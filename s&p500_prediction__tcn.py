# -*- coding: utf-8 -*-
"""S&P500_Prediction__TCN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yw3Iy_wJhSCqd5LuFylNH0hmsPfQraR3

# 1) Installings:
"""

!pip install alpha_vantage pandas numpy yfinance matplotlib tensorflow torch

"""#2) Librarires:"""

from alpha_vantage.timeseries import TimeSeries
import pandas as pd
from alpha_vantage.techindicators import TechIndicators
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

api_key = "ADD_YOUR_API_HERE" #FROM : https://www.alphavantage.co/
ts = TimeSeries(key=api_key, output_format='pandas') # Time Series
ti = TechIndicators(key=api_key, output_format='pandas') # Technical Indicators

# Get daily S&P 500 data (SPY ETF)
data, meta_data = ts.get_daily(symbol='SPY', outputsize='full')
data = data.rename(columns={'1. open': 'open', '2. high': 'high', '3. low': 'low',
                            '4. close': 'close', '5. volume': 'volume'})
data.index = pd.to_datetime(data.index)
data = data.sort_index()
data.head()

from alpha_vantage.techindicators import TechIndicators

ti = TechIndicators(key=api_key, output_format='pandas')

# Fetch 20-day Simple Moving Average (MA)
sma_data, _ = ti.get_sma(symbol='SPY', interval='daily', time_period=20)
sma_data = sma_data.rename(columns={'SMA': 'sma_20'})

# Fetch 14-day Relative Strength Index (RSI)
rsi_data, _ = ti.get_rsi(symbol='SPY', interval='daily', time_period=14)
rsi_data = rsi_data.rename(columns={'RSI': 'rsi_14'})


# Add Bollinger Bands
bbands_data, _ = ti.get_bbands(symbol='SPY', interval='daily', time_period=20)
bbands_data = bbands_data.rename(columns={'Real Upper Band': 'bb_upper', 'Real Middle Band': 'bb_middle', 'Real Lower Band': 'bb_lower'})


# Merge data into a single DataFrame
df = data[['close', 'volume']].join([sma_data, rsi_data, bbands_data], how='inner')
df = df.dropna()  # Remove rows with missing values

df.head()

df['time_idx'] = range(len(df))  # Sequential time index for TFT
df['day_of_week'] = df.index.dayofweek
df['month'] = df.index.month
df['year'] = df.index.year

df.head()

df.to_csv('sp500_data.csv')

"""# Load the Dataset"""

df = pd.read_csv('sp500_data.csv', parse_dates=['date'], index_col='date')
df = df.sort_index()

df.head()

"""# External Data Economic News Sentiment : (NewsAPI)"""

import requests
from textblob import TextBlob

# Fetch news data
news_api_key = "ADD_YOUR_API_HERE" #FROM :https://newsapi.org/
url = f"https://newsapi.org/v2/everything?q=S&P+500&apiKey={news_api_key}"
news_data = requests.get(url).json()

# Check for API errors
if news_data.get('status') != 'ok':
    print(f"Error: {news_data.get('message', 'Unknown error')}")
else:
    # Convert to DataFrame and calculate sentiment
    news_df = pd.DataFrame(news_data['articles'])
    news_df['date'] = pd.to_datetime(news_df['publishedAt'].str[:10])  # Extract date
    news_df['sentiment'] = news_df['title'].apply(lambda x: TextBlob(x).sentiment.polarity)
    daily_sentiment = news_df.groupby('date')['sentiment'].mean().rename('news_sentiment')

"""# External Data :Interest Rates (FRED API)"""

!pip install --upgrade pandas-datareader
!pip install fredapi

import pandas_datareader as pdr
from datetime import datetime
from fredapi import Fred

fred = Fred(api_key='ADD_YOUR_API_HERE')  # FROM :https://fred.stlouisfed.org/docs/api/api_key.html
start_date = '2010-01-01'

# Fetch interest rates
fedfunds = fred.get_series('FEDFUNDS', start_date=start_date).rename('fed_funds_rate')
gs10 = fred.get_series('GS10', start_date=start_date).rename('10yr_treasury_rate')


# Combine into a DataFrame
rates_df = pd.concat([fedfunds, gs10], axis=1)

rates_df.tail()

"""# Merge All Data"""

# Merge sentiment and interest rates with S&P 500 data
df = df.merge(daily_sentiment, left_index=True, right_index=True, how='left')
df = df.merge(rates_df, left_index=True, right_index=True, how='left')

# Forward-fill missing values (interest rates change infrequently)
df[['fed_funds_rate', '10yr_treasury_rate']] = df[['fed_funds_rate', '10yr_treasury_rate']].fillna(method='ffill')

# Fill missing sentiment with neutral (0)
df['news_sentiment'] = df['news_sentiment'].fillna(0)

df.tail()

print(df.dtypes)

"""# Feature Engineering"""

# Lag Features (avoid lookahead bias)
df['close_lag_1'] = df['close'].shift(1)  # Previous day's close
df['close_lag_7'] = df['close'].shift(7)  # Last week's close
df['close_lag_30'] = df['close'].shift(30)
df['close_lag_252'] = df['close'].shift(252)

# Rolling Statistics
df['rolling_7_mean'] = df['close'].rolling(7).mean()  # Weekly average
df['rolling_30_std'] = df['close'].rolling(30).std()   # Monthly volatility

# Lag External Factors
df['sentiment_lag_1'] = df['news_sentiment'].shift(1)
df['fed_funds_lag_1'] = df['fed_funds_rate'].shift(1)
df['10yr_treasury_lag_1'] = df['10yr_treasury_rate'].shift(1)

# Drop rows with NaN (from lags/rolling windows)
df = df.dropna()

"""# Train/Test Split (Time-Based)"""

train_size = int(0.8 * len(df))
train = df.iloc[:train_size]
test = df.iloc[train_size:]

"""# Feature Normalization"""

from sklearn.preprocessing import MinMaxScaler

# Columns to scale (exclude time_idx, target, and categoricals)
scale_cols = ['close', 'volume', 'sma_20', 'rsi_14', 'bb_upper', 'bb_middle', 'bb_lower',
              'news_sentiment', 'fed_funds_rate', '10yr_treasury_rate',
              'rolling_7_mean', 'rolling_30_std',
              'close_lag_1', 'close_lag_7', 'close_lag_30', 'close_lag_252']

scaler = MinMaxScaler(feature_range=(0, 1))
train.loc[:, scale_cols] = scaler.fit_transform(train[scale_cols])
test.loc[:, scale_cols] = scaler.transform(test[scale_cols])

"""# Target Variable"""

# Predict next day's close price
train.loc[:, 'target'] = train['close'].shift(-1)
test.loc[:, 'target'] = test['close'].shift(-1)

# Drop the last row (no target)
train = train.dropna()
test = test.dropna()

df.head()

print(df.dtypes)

"""# TCN

"""

!pip install torch pytorch-lightning

!pip install keras-tcn

!pip install PyWavelets

import torch
import torch.nn as nn
import pytorch_lightning as pl
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import MinMaxScaler
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Dropout
from tcn import TCN
import matplotlib.pyplot as plt

class TCNModel(pl.LightningModule):
    def __init__(self, input_size=5, output_size=1, num_channels=[64, 64, 64], kernel_size=3, dropout=0.2):
        super().__init__()
        self.tcn = nn.Sequential(
            # Layer 1
            nn.Conv1d(input_size, num_channels[0], kernel_size, padding=(kernel_size - 1)),
            nn.ReLU(),
            nn.Dropout(dropout),

            # Layer 2 (dilated)
            nn.Conv1d(num_channels[0], num_channels[1], kernel_size, dilation=2, padding=2*(kernel_size - 1)),
            nn.ReLU(),
            nn.Dropout(dropout),

            # Layer 3 (more dilated)
            nn.Conv1d(num_channels[1], num_channels[2], kernel_size, dilation=4, padding=4*(kernel_size - 1)),
            nn.ReLU(),
            nn.Dropout(dropout),

            # Output
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten(),
            nn.Linear(num_channels[2], output_size)
        )
        self.loss_fn = nn.MSELoss()

    def forward(self, x):
        # x shape: [batch_size, features, sequence_length]
        return self.tcn(x).squeeze(-1)  # Output shape: [batch_size]

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.loss_fn(y_hat, y)
        self.log("train_loss", loss, prog_bar=True)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.loss_fn(y_hat, y)
        self.log("val_loss", loss, prog_bar=True)
        return loss

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.001)

feature_cols = [
    'close', 'volume', 'sma_20', 'rsi_14', 'bb_upper', 'bb_middle', 'bb_lower',
    'time_idx', 'day_of_week', 'month', 'year', 'news_sentiment',
    'fed_funds_rate', '10yr_treasury_rate', 'close_lag_1', 'close_lag_7',
    'close_lag_30', 'close_lag_252', 'rolling_7_mean', 'rolling_30_std',
    'sentiment_lag_1', 'fed_funds_lag_1', '10yr_treasury_lag_1'
]

scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df[feature_cols])

import matplotlib.pyplot as plt
import pywt
import numpy as np

def wavelet_denoise(signal, wavelet='db4', level=1):
    coeff = pywt.wavedec(signal, wavelet, mode="per")
    sigma = np.median(np.abs(coeff[-level])) / 0.6745
    uthresh = sigma * np.sqrt(2 * np.log(len(signal)))
    coeff[1:] = [pywt.threshold(c, value=uthresh, mode='soft') for c in coeff[1:]]
    return pywt.waverec(coeff, wavelet, mode="per")[:len(signal)]  # Match length exactly

denoise_cols = [
    'close', 'volume', 'sma_20', 'rsi_14', 'bb_upper', 'bb_middle', 'bb_lower',
    'close_lag_1', 'close_lag_7', 'close_lag_30', 'close_lag_252',
    'rolling_7_mean', 'rolling_30_std'
]

# Apply denoising
for col in denoise_cols:
    train[col] = wavelet_denoise(train[col].values)
    test[col] = wavelet_denoise(test[col].values)

# Optional: reclip to [0, 1] if you notice the denoised values go out of range
train[denoise_cols] = np.clip(train[denoise_cols], 0, 1)
test[denoise_cols] = np.clip(test[denoise_cols], 0, 1)

def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i+seq_length])
        y.append(data[i+seq_length, 0])  #  target is column 0
    return np.array(X), np.array(y)

seq_lengths = [50, 100, 150, 200]
results = {}

for seq_length in seq_lengths:
    print(f"\nTraining with sequence length: {seq_length}")
    X, y = create_sequences(scaled_data, seq_length)

    # Train-test split
    train_size = int(0.8 * len(X))
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]

    # Convert to tensors and reshape for Conv1D
    X_train = torch.tensor(X_train, dtype=torch.float32).permute(0, 2, 1)
    X_test = torch.tensor(X_test, dtype=torch.float32).permute(0, 2, 1)
    y_train = torch.tensor(y_train, dtype=torch.float32)
    y_test = torch.tensor(y_test, dtype=torch.float32)

    # DataLoaders
    train_dataset = TensorDataset(X_train, y_train)
    test_dataset = TensorDataset(X_test, y_test)
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=64)

    # Init model
    model = TCNModel(
        input_size=X_train.shape[1],
        output_size=1,
        num_channels=[64, 64, 64],
        kernel_size=3,
        dropout=0.2
    )

    # Train
    trainer = pl.Trainer(max_epochs=50, accelerator="auto", enable_progress_bar=False)
    trainer.fit(model, train_loader)

    # Evaluate
    model.eval()
    preds, targets = [], []
    for xb, yb in test_loader:
        with torch.no_grad():
            y_pred = model(xb).squeeze()
            preds.append(y_pred.cpu())
            targets.append(yb.cpu())

    preds = torch.cat(preds).numpy()
    targets = torch.cat(targets).numpy()

    results[seq_length] = (targets, preds)


# plot
fig, axs = plt.subplots(2, 2, figsize=(14, 10))
axs = axs.flatten()

for i, seq_length in enumerate(seq_lengths):
    actual, predicted = results[seq_length]
    axs[i].plot(actual, label="Actual", color="blue", alpha=0.7)
    axs[i].plot(predicted, label="Predicted", color="red", alpha=0.7)
    axs[i].set_title(f"Seq Length = {seq_length}")
    axs[i].legend()
    axs[i].grid(True)

plt.tight_layout()
plt.suptitle("Predictions vs Actuals for Different Sequence Lengths", fontsize=16, y=1.02)
plt.show()

import numpy as np

# Align all predictions to the shortest length
min_len = min(len(v[0]) for v in results.values())

# Stack predictions
all_preds = []
for seq_length in seq_lengths:
    actual, pred = results[seq_length]
    all_preds.append(pred[:min_len])  # Align length

# Compute average prediction
avg_pred = np.mean(np.stack(all_preds), axis=0)
avg_actual = results[seq_lengths[0]][0][:min_len]  # Use any aligned actual

# Plot
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
plt.plot(avg_actual, label="Actual", color="blue", alpha=0.7)
plt.plot(avg_pred, label="Average Prediction", color="green", alpha=0.8)
plt.title("Averaged Predictions from Multiple Sequence Lengths")
plt.xlabel("Time")
plt.ylabel("Close Price (scaled)")
plt.legend()
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 4))
plt.plot(train['close'], label='Denoised Close')
plt.title('Wavelet Denoised Input Feature: Close')
plt.legend(); plt.grid(); plt.tight_layout(); plt.show()





seq_lengths = [30, 50, 100, 200]

def create_multi_scale_sequences(data, seq_lengths):
    X_dict = {seq_len: [] for seq_len in seq_lengths}
    y = []
    max_len = max(seq_lengths)

    for i in range(len(data) - max_len):
        y.append(data[i + max_len, 0])  # Target is always offset by max lookback
        for seq_len in seq_lengths:
            X_dict[seq_len].append(data[i + max_len - seq_len:i + max_len])

    # Convert to arrays and align dimensions
    X_dict = {k: np.array(v) for k, v in X_dict.items()}
    y = np.array(y)
    return X_dict, y

X_dict, y = create_multi_scale_sequences(scaled_data, seq_lengths)

# Train-test split
train_size = int(0.8 * len(y))
X_train_dict = {k: v[:train_size] for k, v in X_dict.items()}
X_test_dict = {k: v[train_size:] for k, v in X_dict.items()}
y_train, y_test = y[:train_size], y[train_size:]

# Convert to tensors
X_train_dict = {k: torch.tensor(v, dtype=torch.float32).permute(0, 2, 1) for k, v in X_train_dict.items()}
X_test_dict = {k: torch.tensor(v, dtype=torch.float32).permute(0, 2, 1) for k, v in X_test_dict.items()}
y_train = torch.tensor(y_train, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32)

import torch
import torch.nn as nn
import torch.nn.functional as F

class Chomp1d(nn.Module):
    def __init__(self, chomp_size):
        super().__init__()
        self.chomp_size = chomp_size

    def forward(self, x):
        return x[:, :, :-self.chomp_size]

class TemporalBlock(nn.Module):
    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout):
        super().__init__()
        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,
                               stride=stride, padding=padding, dilation=dilation)
        self.chomp1 = Chomp1d(padding)
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout)

        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size,
                               stride=stride, padding=padding, dilation=dilation)
        self.chomp2 = Chomp1d(padding)
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout)

        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,
                                 self.conv2, self.chomp2, self.relu2, self.dropout2)
        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None
        self.relu = nn.ReLU()

    def forward(self, x):
        out = self.net(x)
        res = x if self.downsample is None else self.downsample(x)
        return self.relu(out + res)

class TemporalConvNet(nn.Module):
    def __init__(self, input_size, num_channels, kernel_size=2, dropout=0.2):
        super().__init__()
        layers = []
        num_levels = len(num_channels)
        for i in range(num_levels):
            dilation_size = 2 ** i
            in_channels = input_size if i == 0 else num_channels[i-1]
            out_channels = num_channels[i]
            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1,
                                     dilation=dilation_size, padding=(kernel_size-1)*dilation_size,
                                     dropout=dropout)]
        self.network = nn.Sequential(*layers)

    def forward(self, x):  # x shape: (batch, features, seq_len)
        return self.network(x)

class MultiScaleTCN(nn.Module):
    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout, seq_lengths):
        super().__init__()
        self.seq_lengths = seq_lengths
        self.branches = nn.ModuleDict()

        for seq_len in seq_lengths:
            self.branches[str(seq_len)] = TemporalConvNet(
                input_size=input_size,
                num_channels=num_channels,
                kernel_size=kernel_size,
                dropout=dropout
            )

        self.gate = nn.Sequential(
            nn.Linear(len(seq_lengths), len(seq_lengths)),
            nn.Softmax(dim=1)
        )

        self.fc = nn.Linear(num_channels[-1], output_size)

    def forward(self, X_dict, return_weights=False):  # ✅ Add argument here
        outputs = []
        for seq_len in self.seq_lengths:
            x = X_dict[seq_len]
            out = self.branches[str(seq_len)](x)
            out = out[:, :, -1]  # Get last time step
            out = self.fc(out)
            outputs.append(out.unsqueeze(1))  # shape: (batch, 1, output)

        outputs = torch.cat(outputs, dim=1)  # shape: (batch, num_scales, output)
        weights = self.gate(outputs.detach().squeeze(-1))  # (batch, num_scales)

        final_output = torch.sum(weights.unsqueeze(-1) * outputs, dim=1)  # shape: (batch, output)

        if return_weights:
            return final_output, weights
        return final_output

class LitMultiScaleTCN(pl.LightningModule):
    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout, seq_lengths, lr=1e-3):
        super().__init__()
        self.save_hyperparameters()
        self.model = MultiScaleTCN(
            input_size=input_size,
            output_size=output_size,
            num_channels=num_channels,
            kernel_size=kernel_size,
            dropout=dropout,
            seq_lengths=seq_lengths
        )
        self.loss_fn = nn.MSELoss()
        self.lr = lr

    def forward(self, X_dict, return_weights=False):
        return self.model(X_dict, return_weights=return_weights)

    def training_step(self, batch, batch_idx):
        X_dict, y = batch
        y_hat = self(X_dict)
        loss = self.loss_fn(y_hat.squeeze(), y)
        self.log("train_loss", loss)
        return loss

    def validation_step(self, batch, batch_idx):
        X_dict, y = batch
        y_hat = self(X_dict)
        loss = self.loss_fn(y_hat.squeeze(), y)
        self.log("val_loss", loss)

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=self.lr)

from torch.utils.data import Dataset, DataLoader

class MultiScaleDataset(Dataset):
    def __init__(self, X_dict, y):
        self.X_dict = X_dict
        self.y = y
        self.seq_lengths = list(X_dict.keys())
        self.len = y.shape[0]

    def __len__(self):
        return self.len

    def __getitem__(self, idx):
        X_sample = {k: self.X_dict[k][idx] for k in self.seq_lengths}
        y_sample = self.y[idx]
        return X_sample, y_sample

train_dataset = MultiScaleDataset(X_train_dict, y_train)
test_dataset = MultiScaleDataset(X_test_dict, y_test)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(test_dataset, batch_size=64)

model = LitMultiScaleTCN(
    input_size=X_train_dict[30].shape[1],  # features
    output_size=1,
    num_channels=[64, 64, 64],
    kernel_size=3,
    dropout=0.2,
    seq_lengths=seq_lengths,
    lr=1e-3
)

trainer = pl.Trainer(max_epochs=50, accelerator='auto')
trainer.fit(model, train_loader, val_loader)

def get_predictions(model, dataloader):
    model.eval()
    preds, targets = [], []

    with torch.no_grad():
        for batch in dataloader:
            X_dict, y = batch
            y_hat = model(X_dict).squeeze()
            preds.append(y_hat.cpu().numpy())
            targets.append(y.cpu().numpy())

    preds = np.concatenate(preds)
    targets = np.concatenate(targets)
    return preds, targets

preds, targets = get_predictions(model, val_loader)

import matplotlib.pyplot as plt

plt.figure(figsize=(15, 5))
plt.plot(targets, label='Actual', linewidth=2)
plt.plot(preds, label='Predicted', alpha=0.8)
plt.title("Multi-Scale TCN Forecast (Next Day Close)")
plt.xlabel("Time Step")
plt.ylabel("Normalized Close Price")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

def forward(self, X_dict, return_weights=False):
    outputs = []
    for seq_len in self.seq_lengths:
        out = self.branches[str(seq_len)](X_dict[seq_len])
        outputs.append(out.unsqueeze(1))  # (B, 1)

    outputs = torch.cat(outputs, dim=1)  # (B, num_scales)
    weights = self.gate(outputs.detach())

    final_output = torch.sum(weights * outputs, dim=1)

    if return_weights:
        return final_output, weights
    return final_output

def get_predictions_and_weights(model, dataloader):
    model.eval()
    preds, targets, all_weights = [], [], []

    with torch.no_grad():
        for batch in dataloader:
            X_dict, y = batch
            y_hat, weights = model(X_dict, return_weights=True)
            preds.append(y_hat.cpu().numpy())
            targets.append(y.cpu().numpy())
            all_weights.append(weights.cpu().numpy())

    preds = np.concatenate(preds)
    targets = np.concatenate(targets)
    all_weights = np.concatenate(all_weights)
    return preds, targets, all_weights

preds, targets, all_weights = get_predictions_and_weights(model, val_loader)

import seaborn as sns

plt.figure(figsize=(15, 5))
for i, seq_len in enumerate(seq_lengths):
    plt.plot(all_weights[:, i], label=f"{seq_len}-step", alpha=0.8)

plt.title("Dynamic Weighting of Multi-Scale Inputs")
plt.xlabel("Time Step")
plt.ylabel("Gating Weight")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()